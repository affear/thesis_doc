%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% ADOCK
%%--------------------------------------------------------------------------

%% REMEMBER:
%% - section
% \section{Section Title}
% \label{sec:section_title}
%
% - subsection
% \subsection{Subsection Title}
% \label{sub:sub_title}
%
% - paragraph
% \subparagraph{Paragraph Title}
% \label{subp:subp_title}


% --------------------------------------------------------------------------- %
% ---------------------------------- ADOCK ---------------------------------- %
% --------------------------------------------------------------------------- %


\section{Our Solution, aDock}
\label{sec:adock_intro}
The lack of a uniform and standardized test environment for cloud systems brought us to develop aDock.\\
aDock is a suite of tools that lets the final user deploy a complete OpenStack system; run simulations against it; collect output data and view results on a friendly user interface.\\
We chose OpenStack as cloud computing software platform, because of its open-source nature and because of its continuous evolution with the aim of keeping up with the last cloud standards.\\
Our intended users are OpenStack developers who need to run their code in a fully functional environment and researchers who want to try their algorithm on a complete cloud system to test out its behavior.

\section{Requirements}
\label{sec:adock_reqs}
In this section, we will identify both functional and non-functional requirements for aDock.

\subsection{Functional Requirements}
\label{sub:func_req}

\paragraph{FR1}\label{p:fr1} \emph{aDock should provide tools to deploy a complete environment.} \hfill \\
A user should be able to start and update OpenStack's nodes with a single command. aDock should provide the user with an abstraction of a server called ``node''. The ``node'', in its depth, is an Ubuntu based Docker container, shipped with OpenStack services dependencies. A user should be able to decide which services will be installed and started on each node and their internal configuration using a configuration file.
\subparagraph{Solution} FakeStack (see section \ref{sec:fakestack}) is the aDock module which provides the user with the specified tools. Starting a node is as easy as \code{\$ run\_node}. A node can be configured by means of a simple configuration file. Nodes are of two types, \textit{controllers} and \textit{computes}. Controller nodes are different from compute ones because they are shipped with \textit{MySQL} and \textit{RabbitMQ} installations.

\paragraph{FR2}\label{p:fr2} \emph{aDock should provide a tool to run simulations.} \hfill \\
If the user puts his/her code into OpenStack he/she probably aims at running simulations and examine the new piece of code behavior in interacting with the entire system. Simulations should be configurable according to the user needs and repeatable.
\subparagraph{Solution} Oscard (see section \ref{sec:oscard}) is the aDock module which takes care of running repeatable and configurable simulations against an OpenStack system.

\paragraph{FR3}\label{p:fr3} \emph{aDock should persistently store simulations output.} \hfill \\
Once a simulation has been run, it could be interesting to store the outputs of it in terms of generic metrics about the system, such as the average of the number of compute nodes active during the simulation, the average of virtual CPUs used and so on.
\subparagraph{Solution} Oscard, by default, stores the aggregates of a simulation into a Firebase\footnote{\url{https://www.firebase.com/}} backend. In our case, the backend is called Bifrost (see section \ref{sec:others}).

\paragraph{FR4}\label{p:fr4} \emph{aDock should provide a user interface} \hfill \\
Although Firebase provides an interactive user interface, data is displayed in a \textit{JSON} fashion and it is, therefore, not easily understandable and browseable. Simulations results should be displayed to user in a friendly manner, using charts to give the user a glimpse of the current situation. Data representation should be given in real-time.
\subparagraph{Solution} Polyphemus (see section \ref{sec:others}) is the aDock module which takes care of displaying to the user real-time simulation results in a friendly manner.

\paragraph{}\label{p:adock_archi} aDock turns out to be a modular system where each component is configurable and has a precise purpose. FakeStack is employed to start nodes; Oscard runs simulations and collects aggregates on Bifrost; Polyphemus is the eye on the data that shows the user the results obtained.
In figure~\ref{fig:adock_high_arch} we highlight the general architecture of aDock.

\todo{make aDock high-level architecture!}

\subsection{Non-Functional Requirements}
\label{sub:nonfunc_req}
As opposed to functional requirements, aDock has very strong non-functional requirements in order to give a suitable testing environment to our stakeholders. In general we take leverage of Docker and DevStack and use their biggest strength. Docker gives us high speed in running containers and sandboxing by construction and makes aDock cross-platform; while DevStack gives us great flexibility and configurability for what concerns OpenStack services.

\paragraph{NFR1}\label{p:nfr1} \emph{Users should be able to choose which code is running in OpenStack.} \hfill \\
Before booting the entire system the user should be able to choose if he/she wants to run OpenStack code from a precise code repository which is, in general, the better and more supported way to version and share code among developers and even physical machines. Speaking in Git\footnote{\url{http://git-scm.com/}} terms, a user could choose to run the most up to date code (which may be buggy) and so get the code from branch \code{master}, or maybe get a much more stable OpenStack version and get the code from branch \code{stable/juno}. The most interesting fact (and this is the scenario we have in our mind) is that the user could choose to fork an OpenStack service and see his/her code running onto nodes.

\subparagraph{Solution} All of this is achievable thanks to DevStack, which installs OpenStack services cloning repositories from GitHub and running \code{python setup.py install}. By default, DevStack clones official OpenStack repositories from branch \code{master}, but it is possible to specify different repository URLs and branches for each of the OpenStack services by means of \code{local.conf} files.

\paragraph{NFR2}\label{p:nfr2} \emph{aDock should be lightweight.} \hfill \\
Users often need to test algorithms that, by design, target the management and/or optimization of tens of physical servers. Since we can assume that not everyone will have that amount of resources, we believe that aDock should be as light-weight as possible. It should be possible to run aDock on limited hardware, potentially even on one's personal laptop. It is under this assumption that sandboxing becomes important; indeed, the experimentation environment should not have any sort of repercussions on the user's machine; we want the user to be able to build and tear down the environment with no consequences.

\subparagraph{Solution} Docker is a virtualization system which relies on Docker containers which are much more lightweight than virtual machines\footnote{\url{http://devops.com/blogs/devops-toolbox/docker-vs-vms/}} \todo{is the ref authoritative?}. Docker gives us, by construction, speed and lightness.

\paragraph{NFR3}\label{p:nfr3} \emph{The experimentation environment should be highly configurable.} \hfill \\
Our primary goal with aDock is to provide a fast and easy way to create the experimentation environment. We believe that building a system which allows users to design the overall architecture of the cloud system is out of scope of this thesis, mainly because of the intrinsic high complexity and vastness of OpenStack's system itself. Up to now, as a proof of concept, we will focus on ``1 + N'' architecture, with $1$ controller node and $N$ compute nodes.

\subparagraph{Solution} The possibility to configure the system still remains in configuring OpenStack services in terms of their internal behavior. This is achieved, again, thanks to DevStack, which allows us to configure OpenStack in all its aspects through \code{local.conf} file. Each service can be configured in each of DevStack installation phases. Each service, during installation, passes through \textbf{local}, \textbf{pre-install}, \textbf{install}, \textbf{post-config}, \textbf{extra} phases\footnote{\url{http://docs.openstack.org/developer/devstack/configuration.html\#local-conf}}. Configuring a service is as simple as adding few lines to \code{local.conf} file:

\begin{lstlisting}[title=Adding per-service configuration to DevStack's local.conf file]
... # DevStack configurations

[[post-config|\$NOVA-CONF]]
[DEFAULT]
verbose=True
logdir=/var/log/my-nova-logdir

# SCHEDULER
compute_scheduler_driver=nova.scheduler.MyMagicScheduler
# VIRT DRIVER
compute_driver=nova.virt.fake.MyAmazingFakeDriver
\end{lstlisting}


\paragraph{NFR4}\label{p:nfr4} \emph{aDock should allow users to run repeatable simulations.} \hfill \\
It is of paramount importance that users be able to compare their results with baseline approaches, as well as with related work from the state of the art. aDock should make it easy to compare an experiment's results with those of others on the same simulations.

\subparagraph{Solution} Oscard will take into account repeatability both giving the possibility to run the same simulation, at the same time, on multiple hosts, both using pseudo-randomization (see section \ref{sec:oscard}).

% --------------------------------------------------------------------------- %
% -------------------------------- FAKESTACK -------------------------------- %
% --------------------------------------------------------------------------- %

\section{FakeStack}
\label{sec:fakestack}
FakeStack\footnote{\url{https://github.com/affear/fakestack}} is the aDock module which allows the user to manage \textit{nodes}, which are the building blocks of an OpenStack system. Nodes are of two types: \textit{controllers} and \textit{computes}. Both of them are Ubuntu-based Docker containers shipped with pre-installed software that satisfies most\footnote{main services as \textit{Nova}, \textit{Keystone}, \textit{Glance} are actually supported.} of OpenStack's services dependencies. Both controller and compute nodes are configurable by means of simple configuration files ~\ref{sub:fakestack_conf}.\\
FakeStack provides a set of scripts to handle node startup, service updating on live nodes and other features~\ref{sub:fakestack_scripts}.

\subsection{Nodes}
\label{sub:fakestack_scripts}
As already anticipated in requirement~\ref{p:nfr3}, we will focus on ``1 + N'' architectures. This architecture is characterized by $1$ controller node that handles $N$ compute nodes.\\
The main difference between a controller and a compute node is the presence, in the first one, of database (in our case, \textit{MySQL}) and message broker (in our case, \textit{RabbitMQ}) services, compulsory for OpenStack's controller nodes.

To understand how FakeStack really works, it is useful to examine its internal structure:

\begin{figure}[!ht]
\centering{\includegraphics{images/fakestack_tree.png}}
\label{fig:fakestack_tree}
\caption{FakeStack's file structure}
\end{figure}

As we can see in figure~\ref{fig:fakestack_tree}, nodes have two separated \texttt{Dockerfile}s (which makes them two different Docker containers), but they share a set of scripts contained in \texttt{shared} folder:

\begin{description}
	\item[cmd.sh] This is the script that will be run when the container starts (\code{docker run}). In algorithm~\ref{alg:cmd.sh} we explain its behavior in terms of pseudo-code.
	\begin{algorithm}[H]
	\caption{\texttt{cmd.sh} behavior}
	\label{alg:cmd.sh}
	% 1 is for "number any line", 0 for "no"
	\begin{algorithmic}[0]
		\If{node is \emph{controller}}
			\State set last IP in Docker bridge \Comment{assign static IP address to \emph{controller}}
		\EndIf

		\State ping 8.8.8.8 \Comment{Check internet connection}

		\If{node is \emph{controller}}
			\State start \texttt{mysql}
			\State start \texttt{rabbitmq-server}
		\EndIf

		\State \textbf{./stack.sh} \Comment{real OpenStack installation (using DevStack)}

		\State /bin/bash \Comment{let the user work on the container}
	\end{algorithmic}
	\end{algorithm}
	
	\item[reinstall\_service.sh] This script allows the user to update a service on this node specifing its name (e.g. \texttt{nova}, \texttt{glance} and so on)

	\item[update\_quotas.sh] This script allows the user to enlarge \textit{quotas} for the \textit{tenant}\footnote{\textit{Quotas} are limits on how much CPU, memory and disk space the tenant can use. \textit{Tenant} is an OpenStack concept similar to Linux groups.} in use (in our case \texttt{admin}) to a very big amount. Its usage is justified by the fact that probably the user will spawn hundreds or thousands of virtual machines on its OpenStack nodes and, normally, standard quotas will prevent him from doing so. Enlarging quotas is an easy and fast way to allow the user not to worry about how many virtual machines he owns.
\end{description}

Once a node has been built, all of this shared scripts are copied to the file-system of the node.\\
\texttt{local.conf} file, instead, is bound\footnote{\texttt{local.conf} is a \href{https://docs.docker.com/userguide/dockervolumes/\#data-volumes}{\textit{Data Volume}}. Basically it is a file whose modifications are visible at each container's run.} to node's file-system avoiding rebuilding at each modification.

When a node is started, \texttt{cmd.sh} will run, resulting in running DevStack's \texttt{stack.sh}. At this very moment OpenStack's installation starts~\ref{} \todo{reference to DevStack's detailed description}.

\subsection{Scripts}
\label{sub:fakestack_scripts}
Once a user enters FakeStack root directory he/she has to \code{source fakerc}. Executing this command, all of the scripts contained in \texttt{scripts} directory become available in \texttt{PATH}. The prefix \texttt{ftools\_} is added to avoid conflicts in names.\\
Scripts for running and updating nodes takes leverage of Linux screens\footnote{\url{http://linux.die.net/man/1/screen}}. Screens are powerful tools to run detached shells from within another shell. This feature gives lots of advantages both in terms of ease of use and in running long running jobs via SSH.\\
All of aDock processing is confined into two different screens, \texttt{running} and \texttt{updating}. Thanks to this, the user will not be obliged to open lots of shells, but he could use only one; keep it clean from computation and reattach to aDock screens once needed. If the user wants to run a long running task (e.g. a very long simulation or lots of different, small simulations) on a remote server via SSH, he will not need to keep the SSH session opened and wait for simulations to end; the screen session, in fact, will stay open (and so the processes within it, running) indipendently from SSH connection. 

We list here and describe scripts contained into \texttt{scripts} directory.
\begin{description}
	\item[createbr] Input: bridge name. Creates a bridge with CIDR $42.42.0.0/24$ named as the bridge name passed as first argument by the user. This bridge is intended to be used by Docker, setting the option \code{-b <bridge\_name>} into \texttt{/etc/default/docker}. Run this script before starting the system or edit IP configuration in \texttt{cmd.sh}. In fact, \texttt{cmd.sh} will set controller's IP to the last available into that precise network ($42.42.255.254$). After running this script, Docker service has to be restarted.
	\item[destroyall] Stops and destroys all OpenStack nodes.
	\item[runctrl] Runs one controller node on a new window in screen \texttt{running}.
	\item[runcmps] Input: \texttt{N}. Starts \texttt{N} compute nodes on a new window in screen \texttt{running}. It is necessary to confirm the operation in the window created. Once confirmed, a new window, attached to one of the compute nodes started, will be opened.
	\item[screenbyname] Input: screen name. Reattachs to the screen named as given by the user, if it exists.
	\item[updateall] Input: service name. Updates the service given by the user on all OpenStack nodes. All of the processing is performed into screen \texttt{updating}.
\end{description}

We list here and describe scripts ``sourced'' from \texttt{fakerc} (\texttt{ftools\_} convention is always maintained).
\begin{description}
	\item[runcmp] Alias for \code{ftools\_runcmps 1}.
	\item[build] Input: \texttt{ctrl} or \texttt{cmp}. This script builds the node, and so, regenerates it from a pure Ubuntu image. It is necessary to run this script only in case that some of the files (apart from \texttt{local.conf}) has been modified.
	\item[attach] Input: container ID. Attaches to a Docker container. Alias for \code{docker attach <container\_id>}.
\end{description}

\subsection{Configuration}
\label{sub:fakestack_conf}
\todo{repair listings placement!}

Fakestack takes leverage of the powerful configurable options of DevStack. Modifying \texttt{local.conf} files before starting a node, it is possible to change enabled services (see listing~\ref{lst:enabled_services}) and their internal configuration (see listing~\ref{lst:driver_conf}). Every service is configurable in each of its installation \textit{phases}. Configuring it is as simple as adding a \code{[[ <phase> | <config-file-name> ]]} line (e.g. \code{[[post-config|\$GLANCE\_CONF]]}) to \texttt{local.conf} file and add configuration options below. DevStack's service installation phases are \textbf{local}, \textbf{pre-install}, \textbf{install}, \textbf{post-config}, \textbf{extra}\footnote{For more information see \url{http://docs.openstack.org/developer/devstack/configuration.html\#local-conf}}.\\
Most important, it is possible to choose a different Git repository and Git branch for each of OpenStack's services enabled (see listing~\ref{lst:change_repo}). DevStack, in fact, install services \textit{cloning} those repositories and running \code{python setup.py install}\footnote{It is the standard way to install \textit{PyPI} packages. More information can be found at \url{https://wiki.python.org/moin/CheeseShopTutorial}}.\\
Thanks to this important piece of configuration, a user can \textit{fork} an OpenStack project; develop its code and use its new forked repository URL in DevStack's configuration.\\
In listing~\ref{lst:localconf_ex} we show a possible complete example of \texttt{local.conf} file for a compute node.

\begin{lstlisting}[float, floatplacement=H, caption={Choose OpenStack's enabled services}, label={lst:enabled_services}, numbers=none]
... # Other configuration options

# Enables:
# - Nova Compute
# - Nova API
# - Nova Network
ENABLED_SERVICES=n-cpu,n-api,n-net

... # Other configuration options
\end{lstlisting}

\begin{lstlisting}[float, floatplacement=H, caption={Internal configuration of Nova}, label={lst:driver_conf}, numbers=none]
... # Other configuration options

[[post-config|$NOVA_CONF]]
[DEFAULT]
compute_driver=nova.virt.fake.MyFakeDriver

... # Other configuration options
\end{lstlisting}

\begin{lstlisting}[float, floatplacement=H, caption={Change repository URL}, label={lst:change_repo}, numbers=none]
... # Other configuration options

NOVA_REPO=https://github.com/me/nova.git
NOVA_BRANCH=my-branch

... # Other configuration options
\end{lstlisting}

\begin{lstlisting}[float, floatplacement=H, caption={Complete \texttt{local.conf} example for compute node}, label={lst:localconf_ex}]
[[local|localrc]]
FLAT_INTERFACE=eth0
MULTI_HOST=1
LOGFILE=/opt/stack/logs/stack.sh.log
SCREEN_LOGDIR=$DEST/logs/screen

NOVA_REPO=https://github.com/me/nova.git
NOVA_BRANCH=my-branch

DATABASE_TYPE=mysql

ADMIN_PASSWORD=pwstack
MYSQL_PASSWORD=pwstack
RABBIT_PASSWORD=pwstack
SERVICE_PASSWORD=pwstack
SERVICE_TOKEN=tokenstack

SERVICE_HOST=controller
MYSQL_HOST=controller
RABBIT_HOST=controller

NOVA_VNC_ENABLED=False
VIRT_DRIVER=fake

ENABLED_SERVICES=n-cpu,n-api,n-net

[[post-config|$NOVA_CONF]]
[DEFAULT]
compute_driver=nova.virt.fake.MyFakeDriver
\end{lstlisting}

\subsection{Example}
\label{sub:fakestack_ex}

% --------------------------------------------------------------------------- %
% ---------------------------------- OSCARD --------------------------------- %
% --------------------------------------------------------------------------- %

\section{Oscard}
\label{sec:oscard}

% --------------------------------------------------------------------------- %
% --------------------------- BIFROST + POLYPHEMUS -------------------------- %
% --------------------------------------------------------------------------- %

\section{Other Components}
\label{sec:others}