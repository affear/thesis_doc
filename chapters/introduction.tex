%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% INTRODUCTION
%%--------------------------------------------------------------------------


The rapid growth of cloud services --in the past years-- has driven a steep rise in the number of massively-scaled data centers. As a consequence energy consumption of data centers has become an urgent and important problem, as the power they need has reached the $1.3\%$ of the world’s total in 2010\cite{Koomey:2011vr}. This is why an intelligent and efficient strategy for resource allocation is critical, to try to make the most of the available hardware.

Within an Infrastructure as a Service (IaaS) one way to achieve this goal is to try to minimize the number of running servers, while maintaining all the requested virtual machines running and available. This is commonly done by placing VMs intelligently on available servers. Exploiting this solution, it is possible to ensure that the data center is ``filled'' in a consistent way, avoiding under-allocated resources.\\
The problem with this solution is that it does not cover the cases in which VMs are deallocated from the hosting hardware. In these cases, in fact, the system could reach a state in which various --or all-- the data center servers are used inefficiently, leaving some of them under-utilized and consuming more power because some of them may be potentially turned off.\\
To address this problem it is possible to periodically \textit{consolidate} the arrangement of VMs within the data center, migrating them from under-utilized servers to servers which can host them; thanks to these migrations it might be possible to ``empty'' the under-utilized machines and take them into an energy-saving power state, e.g., as a deep-sleep. As illustrated in the State of the Art (see Chapter 3), during the past years VM consolidation has gained more and more attention from the community, and a lot of algorithms and techniques have been proposed to address it.\\
Despite it being a very interesting approach to power saving, we lack solid VM Consolidation implementations, especially in non-proprietary IaaS; in fact, most of the solutions from the state of the art are theoretical ---without practical tests in real environments. Take OpenStack, for example. It is the most important and used open-source IaaS solution, yet it doesn’t provide any official implementation of the concept of VM consolidation.

In this thesis we decided to try to apply the concept of VM consolidation to OpenStack, since it is the reference platform for IaaS in the open-source world and it has a large and active community. More specifically, our goal was to implement a new OpenStack module that would allow us to ``plug-in'' and test a number of consolidation algorithms, and to see their impact on a real cloud system. At the beginning, however, we faced the problem of how to run, test and benchmark our code in an OpenStack environment. In order to deal with aspects like Scheduling, Virtual Machine Placement, and Server Consolidation, we needed a highly configurable system that would allow us to run simulations and benchmarks to evaluate the soundness of our solutions. Unfortunately we had limited server hardware, and could not construct a realistic testbed.

By looking at the state of the art we found that we were not alone; limited hardware is indeed a common barrier to experimenting with cloud infrastructure. Although OpenStack can be used to create testbeds, it is not uncommon in literature to find works that are plagued by unrealistic setups that use only a handful of servers.\\
For these reasons we decided to shift the focus of our work from VM consolidation algorithms to the implementation of a set of tools that would enable us to easily setup a cloud test environment. We needed a quick and easy way to install and deploy code into a realistic experimentation environment, for example, to test a new consolidation algorithm. Moreover the experimentation environment would have to be lightweight, in order to allow it to be used with limited hardware resources (e.g., on a single developer workstation), and highly configurable to meet the needs of different situations.\\
However, setting up a testbed is necessary but not sufficient. One must also be able to create repeatable experiments that can be used to compare one’s results to baseline or related approaches from the state of the art. We needed a way to automatically simulate, in a repeatable way, the workload generated from user applications that normally run on a cloud environment. Moreover we realized that it would be very useful to show real-time data of the simulations to analyze the behavior of the system in different configurations and analyze and compare the data collected.

Our solution to these problems was to develop aDock, a suite of tools for creating cloud infrastructure experimentation environments that are lightweight, sandboxed, and configurable. Our goal is to provide developers, sysadmins, and researchers a simple solution through which they can easily access a fully functional cloud installation of OpenStack.

aDock is made up of four main components: FakeStack (see \ref{sec:fakestack}), Oscard (see \ref{sec:oscard}), Bifrost (see \ref{sub:bifrost}), and Polyphemus (see \ref{sub:polyphemus}). FakeStack allows the user to manage the deployed OpenStack system, making it possible to configure and install one with the minimum effort. Oscard allows the user to configure and run repeatable simulations through command-line tools, as well as to store the experiments' outputs on Bifrost, i.e., our simulation database. Finally, Polyphemus allows the user to follow a simulation's progress in real-time, and to analyze relevant data such as the environment's resource utilization.\\
FakeStack, Oscard, and Polyphemus take advantage of Docker (see section \ref{sub:sota_docker} for more details), an open platform that exploits Linux containers to virtualize Operating Systems and Applications on a host; it allowed us to keep the overall solution lightweight, as well as to keep deployment quick thanks to its low resource requirements, especially compared to more traditional Virtual Machines.

As a further contribution we used aDock to create a Virtual Machine consolidation module for OpenStack. The module provides extension points that make it easy to ``plug-in'' any consolidation algorithm we may want to test. To do this the module provides an abstract view of the experimentation environment and of its resource usage, allowing the more inexperienced developer to test his/her algorithms without having to fully understand OpenStack's complex internal behavior.

In order to prove the soundness of our solution, we implemented four different consolidation algorithms: Random, Genetic, Genetic ``best'', and Holistic; see \ref{sec:cons_algs} for more details. Using aDock, we were able to deploy a ``1 + 5'' OpenStack system ---that is one containing one \textit{controller node} and $5$ \textit{compute node}s. The compute nodes are the ones that actually host virtual machines. The controller node, instead, manages all of them.\\
This was achieved in a reasonable time on a laptop with very limited hardware resources ---without compromising its usage. On a moderate Server machine we were even able to achieve a ``1 + 42'' configuration. All consolidation algorithms brought a significant improvement to OpenStack's resource usage. In the bast case we were able to achieve a $14\%$ increase in vCPU usage, a $20\%$ increase in RAM usage, a $1.5\%$ increase in disk usage, and a decrease of about $30\%$ active nodes. 
Both aDock and our Consolidator module open the door to a lot of interesting future work. For what concerns aDock, Oscard will continue to grow to become a more and more realistic solution for running simulations. For example, we could exploit data from the state of the art to extract more realistic ratios between user actions (e.g. creating vs destroying Virtual Machines), user action density over time, and different application-specific virtual machine workloads (e.g. web applications, compute intensive tasks, etc.).

We would also like to make FakeStack more \emph{modular}. Each OpenStack service could be dockerized into a single container, thus, making FakeStack even more flexible in terms of possible experimentation environment configurations. Up until now, more OpenStack services run in a single Docker container, making it harder to relize some specific OpenStack architectures. We are also interested in providing ``ready-to-go'' compositions of containers, which would make it even easier for the user to deploy an entire OpenStack system.

Regarding Virtual Machine consolidation in OpenStack, we believe our consolidator can provide a valid testbed for comparing approaches from literature (see sections \ref{sec:sota_game_theroy} and \ref{sec:sota_mutli_agent}). In the future we might be inetersted in providing a detailed survey of existing approaches, and base it on our consolidator module.