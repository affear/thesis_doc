%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% INTRODUCTION
%%--------------------------------------------------------------------------


The rapid growth of cloud services, in the past years, has heavily raised the number of data centers all over the world. Nowadays, energy consumption of data centers has become an urgent and important problem, as the power they need has reached the \% of the world’s total in \todo{\% year and reference}. This is why an intelligent and efficient strategy for resource allocation is critical, to try to make the most of available hardware.\\
Within an Infrastructure as a Service (IaaS) one way to achieve this goal is to try to minimize the number of running servers, while maintaining all the requested virtual machines running and available. This is commonly done by placing VMs intelligently on available servers. Exploiting this solution, it is possible to ensure that the data center is ``filled'' in a consistent way, avoiding under-allocated resources.\\
The problem with this solution is that it does not cover the cases in which VMs are deallocated from the hosting hardware. In these cases, in fact, the system could reach a state in which various --or all-- the data center servers are used inefficiently, leaving some of them under-utilized and consuming more power because some of them may be potentially turned off.\\
To address this problem it is possible to periodically \textit{consolidate} the arrangement of VMs within the data center, migrating them from under-utilized servers to servers which can host them; thanks to these migrations it might be possible to ``empty'' the under-utilized machines and take them into an energy-saving power state, e.g., as a deep-sleep. As illustrated in the State of the Art (see Chapter 3), during the past years VM consolidation has gained more and more attention from the community, and a lot of algorithms and techniques have been proposed to address it.\\
Despite it being a very interesting approach to power saving, we lack VM Consolidation implementations, especially in non-proprietary IaaS; in fact, most of the existing solutions are theoretical without practical tests in real environments. Take OpenStack, for example. It is the most important and used open-source IaaS solution; yet it doesn’t provide any official implementation of the concept of VM consolidation.\\
We decided to focus on OpenStack, since it is the reference platform for IaaS in the open-source world and it has a large and active community, and try to apply the concept of VM consolidation to it.\\ 
When starting this thesis our goal was to implement a new module for OpenStack that would allow us to ``plug-in'' and test a number of consolidation algorithms, to see their impact on a real cloud system. At the beginning, however, we faced the problem of how to run, test and benchmark our code in an OpenStack environment. In order to deal with aspects like Scheduling, Virtual Machine Placement, and Server Consolidation, we needed an highly configurable system that would allow us to run simulations and benchmarks to evaluate the soundness of our solutions. Unfortunately we had limited server hardware, and could not construct a realistic testbed.\\
Limited hardware availability is common barrier to experimenting with cloud infrastructure. Although OpenStack can be used to create testbeds, it is not uncommon in literature to find works that are plagued by unrealistic setups that use only a handful of servers.\\
For these reasons we decided to shift the focus of our work from VM consolidation algorithms to the implementation of a set of tools that would have enabled us to easily setup a cloud test environment to test them. We needed a system quick and easy to install and deploy in order to reduce the effort, for example, to test an algorithm. Moreover the system would have to be lightweight in order to be used with limited hardware resources (for example to be used on a developer workstation), and highly configurable to meet the needs of different situations.\\
However, setting up a testbed is necessary but not sufficient. One must also be able to create repeatable experiments that can be used to compare one’s results to baseline or related approaches from the state of the art. We needed a way to automatically simulate, in a repeatable way, the workload generated from user applications that normally run on a cloud environment. Moreover we realized that it would be very useful to show real-time data of the simulations to analyze the behavior of the system in different configurations and analyze and compare the data collected.\\
Therefore we decided to develop aDock, a suite of tools for creating performance, sandboxed, and configurable cloud infrastructure experimentation environments that developer, sysadmins and researchers can exploit to access a fully functional cloud installation of OpenStack.

aDock is composed of four principal modules: FakeStack (see \ref{sec:fakestack}), Oscard (see \ref{sec:oscard}), Bifrost (see \ref{sub:bifrost}), and Polyphemus (see \ref{sub:polyphemus}).
FakeStack allows the user to manage the OpenStack system and makes it possible to configure and install it with the minimum effort. Oscard is developed to allow the user to configure and run repeatable simulations through command-line tools as well as to store the outputs data on Bifrost (the simulations database).
Finally Polyphemus allows the user to follow in real-time the progress of the simulation and to analyze various relevant data such as resource utilization or the characteristics of the simulation.
FakeStack, Oscard, and Polyphemus (Bifrost exploits an online service called Firebase, for more details see \ref{sub:bifrost}) take advantage of Docker (see section \ref{sub:sota_docker} for more details), an open platform that exploits Linux containers to virtualize Operating Systems and Applications on a host; it allowed us to to maintain all the system lightweight and quick to deploy thanks to its low resource requirements compared to Virtual Machines.

Thanks to aDock, we introduced a service for virtual machine consolidation in OpenStack. The service provides extension points that makes possible to the user to write algorithms that address VM consolidation simply implementing a python method. The service provides the user with an abstraction of the system, giving him/her all the ``tools'' needed to develop code in OpenStack without even knowing its internal behavior.\\
Moreover, we implemented four different types of consolidators (Random, Genetic, Genetic ``best'', and Holistic; see \ref{sec:cons_algs} for more details) to prove the soundness of our solution.

Using aDock, we were able to deploy a ``1 + 5'' OpenStack system (that is one \textit{controller node} and $5$ \textit{compute node}; see \ref{sec:openstack} for more informations) in a reasonable time on a laptop with limited hardware resources without compromising its usage. On a moderate Server machine we even achieved a ``1 + 42'' configuration.

All consolidation algorithms brought to a significant improvement in OpenStack system's resource usage. The best algorithm found, in fact, brought to about $14\%$ increase in vCPUs usage; about $20\%$ increase in RAM usage; about $1.5\%$ increase in disk usage and a decrease of about $30\%$ active nodes.

aDock and consolidation topics leave lots of future work possibilities. For what concerns aDock, Oscard could become more and more realistic in running simulations. We could take data from the state of the art and extract ratios between different user actions (e.g. creating a virtual machine vs destroying one); user actions' density throughout time, and different application-specific virtual machine workloads (e.g. web applications, compute intensive tasks, etc.).\\
We could make FakeStack more \emph{modular}. Each OpenStack service could be dockerized into a single container, thus, making FakeStack more and more flexible. Up to now, more services run in a single container making it difficult for FakeStack to be configured in terms of different OpenStack architectures. We could then provide the user with ``ready-to-go'' compositions of containers to make it easier for the user to deploy an entire OpenStack system.

For what concerns virtual machine consolidation in OpenStack, better consolidation algorithms could be implemented. They could be taken from literature (see sections \ref{sec:sota_game_theroy} and \ref{sec:sota_mutli_agent}) or custom.