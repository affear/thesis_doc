%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% CONCLUSIONS
%%--------------------------------------------------------------------------

Due to the two-topic nature of this thesis we split this chapter into two sections. Section \ref{sec:conc_adock} discusses about conclusions on results obtained from tests on aDock system (see section \ref{sec:eval_adock}) and possible future work on the system itself. Section \ref{sec:conc_cons} discusses conclusions on results obtained from consolidation algorithms testing (see section \ref{sec:eval_cons}) and future work in the field of virtual machine consolidation in OpenStack.

\section{aDock}
\label{sec:conc_adock}
Tests conducted on aDock system confirmed our suppositions on aDock. It is possible for a simple developer or researcher to develop on its laptop and run simulations against an OpenStack system using aDock. Results obtained give reasonable starting times for the entire architecture. We can say that aDock is ``lightweight''. However we think that a comparison between aDock and one of the other options available at the state of the art (e.g. \textit{Chef}. See subsection \ref{sub:sota_chef}) would be a must. Direct comparison is necessary to understand if aDock is really better then its ``competitors''. To our detriment there is to say that, by construction, containers make aDock more lightweight than an architecture with hypervisor and virtual machines (see paragraph \ref{p:nfr2}) \todo{``figura dei cioccolatai''??}.

Future work on aDock is vast. For what concerns FakeStack, it could become a \emph{modular} system. Docker developers strongly advocate small, lightweight containers where each container has a single responsibility. This is not the case in FakeStack, which gives a lot of responsibilities to a single container. FakeStack nodes are ``fat containers'' that run a lot of different processes. The controller node, for example, in its minimal configuration, runs \texttt{rabbitmq-server}; \texttt{mysql}; \texttt{keystone}; \texttt{glance-api}; \texttt{glance-registry}; \texttt{nova-api}; \texttt{nova-cert}; \texttt{nova-conductor} and \texttt{nova-scheduler}. This is in total contrast with Docker philosophy and makes impossible for FakeStack to be ``flexible''. The solution to this problem would be to make each OpenStack service run in a separate container\footnote{There is already an attempt to this, \url{https://hub.docker.com/u/cosmicq/}.}. This change would make FakeStack much more flexible and configurable by the user. In this case we should provide a templating language to make FakeStack automatically deploy an OpenStack architecture as given by the user, as \textit{Chef} and \textit{Puppet} already do. According to us and with the adequate support by the OpenStack community, FakeStack could become the equivalent, but Docker-powered, of \textit{Chef-OpenStack} and \textit{Puppet-OpenStack} in the OpenStack world.

For what concerns the templating language and automate deploying of an OpenStack system, Docker recently released tools for container orchestration\footnote{\url{http://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/}} among which we can find \textit{Compose}\footnote{\url{http://docs.docker.com/compose/}} which is ``a way of defining and running multi-container distributed applications with Docker''. We think that this functionality fits perfectly with what we would need into FakeStack. Compose allows the user to create a \texttt{docker-compose.yml} file and start its newly defined system running \code{docker-compose up} and Compose will start and run the entire system, determining the right order to start containers.

If we want to start a controller node, we could start it using Compose. Listing \ref{lst:compose_ctrl} shows a possible \texttt{docker-compose.yml} file for a controller node\footnote{Not all necessary services are listed. Ports are avoided. Images are supposed to be available. Listing \ref{lst:compose_ctrl} is shown only as a proof of concept.}. Keystone (\texttt{key}) container depends on MySQL (\texttt{db}) and RabbitMq (\texttt{rabbit}) containers, as well as Glance API (\texttt{g-api}) and Nova API (\texttt{n-api}) containers depend on \texttt{key}. All configuration files are specified as volumes to make it unnecessary to rebuild the container if there is a modification into configuration.

\begin{lstlisting}[
	float,
	caption={Sample controller's \texttt{docker-compose.yml}},
	label={lst:compose_ctrl},
	tabsize=2,
	numbers=none
]
key:
  image: fs-key
  links:
   - db
   - rabbit
  ports:
   - ...
  volumes:
   - ./keystone.conf

g-api:
	image: fs-g-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./glance.conf

n-api:
	image: fs-n-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./nova.conf

db:
  image: mysql

rabbit:
  image: rabbit
\end{lstlisting}

We could also provide the user with built-in \emph{composed} system, such as ``all-in-one'' and ``1 + N'' architectures. We could also provide systems for single OpenStack modules. ``Nova'' composition, for example, could include containers for all Nova services, such as the scheduler, the API and so on.

For what concerns Oscard, we could give the possibility to user to run simulation with a composition of \emph{all} possible operations that OpenStack allows to perform. Only create, resize and destroy operations are supported up to now. Another point is to support aggregates of simulations. The user should be able to get averages on groups of simulations and label them. \todo{more}

\section{Consolidators}
\label{sec:conc_cons}