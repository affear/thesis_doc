%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% CONCLUSIONS
%%--------------------------------------------------------------------------

Due to the two-topic nature of this thesis we split this chapter into two sections. Section \ref{sec:conc_adock} discusses about conclusions on results obtained from tests on aDock system (see section \ref{sec:eval_adock}) and possible future work on the system itself. Section \ref{sec:conc_cons} discusses conclusions on results obtained from consolidation algorithms testing (see section \ref{sec:eval_cons}) and future work in the field of virtual machine consolidation in OpenStack.

\section{aDock}
\label{sec:conc_adock}
Tests conducted on aDock system confirmed our suppositions on aDock. It is possible for a simple developer or researcher to develop on its laptop and run simulations against an OpenStack system using aDock. Results obtained give reasonable starting times for the entire architecture. We can say that aDock is ``lightweight''. However we think that a comparison between aDock and one of the other options available at the state of the art (e.g. \textit{Chef}. See subsection \ref{sub:sota_chef}) would be a must. Direct comparison is necessary to understand if aDock is really better then its ``competitors''. To our detriment there is to say that, by construction, containers make aDock more lightweight than an architecture with hypervisor and virtual machines (see paragraph \ref{p:nfr2}) \todo{``figura dei cioccolatai''??}.

Future work on aDock is vast. For what concerns FakeStack, it could become a \emph{modular} system. Docker developers strongly advocate small, lightweight containers where each container has a single responsibility. This is not the case in FakeStack, which gives a lot of responsibilities to a single container. FakeStack nodes are ``fat containers'' that run a lot of different processes. The controller node, for example, in its minimal configuration, runs \texttt{rabbitmq-server}; \texttt{mysql}; \texttt{keystone}; \texttt{glance-api}; \texttt{glance-registry}; \texttt{nova-api}; \texttt{nova-cert}; \texttt{nova-conductor} and \texttt{nova-scheduler}. This is in total contrast with Docker philosophy and makes impossible for FakeStack to be ``flexible''. The solution to this problem would be to make each OpenStack service run in a separate container\footnote{There is already an attempt to this, \url{https://hub.docker.com/u/cosmicq/}.}. This change would make FakeStack much more flexible and configurable by the user. In this case we should provide a templating language to make FakeStack automatically deploy an OpenStack architecture as given by the user, as \textit{Chef} and \textit{Puppet} already do. According to us and with the adequate support by the OpenStack community, FakeStack could become the equivalent, but Docker-powered, of \textit{Chef-OpenStack} and \textit{Puppet-OpenStack} in the OpenStack world.

For what concerns the templating language and automate deploying of an OpenStack system, Docker recently released tools for container orchestration\footnote{\url{http://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/}} among which we can find \textit{Compose}\footnote{\url{http://docs.docker.com/compose/}} which is ``a way of defining and running multi-container distributed applications with Docker''. We think that this functionality fits perfectly with what we need into FakeStack. Compose allows the user to create a \texttt{docker-compose.yml} file and start its newly defined system running \code{docker-compose up} and Compose will start and run the entire system, determining the right order to start containers.

If we want to start a controller node, we could start it using Compose. Listing \ref{lst:compose_ctrl} shows a possible \texttt{docker-compose.yml} file for a controller node as a proof of concept\footnote{Not all necessary services are listed. Ports are avoided. Images are supposed to be available.}. Keystone (\texttt{key}) container depends on MySQL (\texttt{db}) and RabbitMQ (\texttt{rabbit}) containers, as well as Glance API (\texttt{g-api}) and Nova API (\texttt{n-api}) containers depend on \texttt{key}. All configuration files are specified as volumes to make it unnecessary to rebuild images if a modification into configuration happens.

\begin{lstlisting}[
	float,
	caption={Sample controller's \texttt{docker-compose.yml}},
	label={lst:compose_ctrl},
	tabsize=2,
	numbers=none
]
key:
  image: fs-key
  links:
   - db
   - rabbit
  ports:
   - ...
  volumes:
   - ./keystone.conf

g-api:
	image: fs-g-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./glance.conf

n-api:
	image: fs-n-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./nova.conf

db:
  image: mysql

rabbit:
  image: rabbit
\end{lstlisting}

We could also provide the user with built-in \emph{composed} system, such as ``all-in-one'' and ``1 + N'' architectures. We could also provide systems for single OpenStack modules. ``Nova'' composition, for example, could include containers for all Nova services, such as the scheduler, the API and so on.

In modular case, every image maps to a single OpenStack service. Images provided should fit users needs to choose OpenStack's running code (as already said in non-functional requirement $1$. See \ref{p:nfr1}). For this reason we think that we could use DevStack again to install each single service. This choice would allow the user to choose GitHub repository URL and branch and to configure the service itself (see subsection \ref{sub:fakestack_conf}). The user should create a different configuration file for each service (e.g. \texttt{keystone.conf} and \texttt{nova.conf}) containing the repository URL and branch used and the different installation phases of the service with their specific configuration. At this point, we could merge each different configuration file to a main \texttt{local.conf} file which specifies the different \texttt{ENABLED\_SERVICES}, in this case only $1$, for each of the images.

OpenStack configuration is not ``hot-reloaded'' at every modification, thus, implies container reboot. We could avoid rebooting (rebooting is heavier then service restarting, which would imply a new DevStack installation) providing scripts to restart services inside containers. This fact is not trivial, because services could have dependencies among them and restarting could break service startup and other related services. 

For what concerns Oscard, we could give the possibility to user to run simulations with a composition of \emph{all} possible operations that OpenStack allows to perform. Only create, resize and destroy operations are supported up to now. Another point is to support aggregates of simulations. It happens that a user wants to run a group of simulations and extract averages of the aggregates already stored in Bifrost by Oscard. It was our case when we came to groups of $50$ simulations, each block with a different consolidator. To calculate the numbers in table \ref{tab:cons_vs}, we wrote a python script which extracted averages of aggregates from each group of simulations, knowing starting and ending simulation ID of each group. We suppose that a situation like this one could happen very often to users. Oscard should allow the user to give an unique label to a group of simulations and automatically extract the averages (also standard deviations would be good) of the aggregates which Oscard already calculates. Polyphemus should display those new data and represent someway the concept of \emph{group} of simulations, of course.

\section{Virtual Machine Consolidation in OpenStack}
\label{sec:conc_cons}
Tests run on different consolidators confirmed what we thought about OpenStack's virtual machine consolidation. This practice introduces high resource usage improvements with respect to ``vanilla'' OpenStack and so, it could lead to energy saving in the whole system. The best algorithm found, in fact, brought to about $14\%$ increase in vCPUs usage; about $20\%$ increase in RAM usage; about $1.5\%$ increase in disk usage and a decrease of about $3$ active nodes and so, of a $30\%$ (on $10$ total nodes) with a maximum downscale time of about $10\%$.

In the future, we will extract standard power on and power off timings of different servers from the state of the art and compare them with maximum downscale times obtained. Maximum downscale time, in fact, is intended to be compared with those timings. When a nodes is inactive it can be powered off. If the time in which the node is inactive is too short, it could be a nonsense to turn it off, because the system could need it while this is happening. It could be that some algorithms doesn't allow power off and so, they give a ``fake'' improvement. Resource usage increases, but the system cannot exploit this efficiency trait being impossible for it to turn a server off.

More efficient consolidation algorithms could be implemented in the future. The state of the art gives a lot of hints about this (see section \ref{sec:sota_game_theroy} and section \ref{sec:sota_mutli_agent}). 

Oscard could run more realistic simulations. We could get data from different real cloud systems to understand how many operations are performed per second; their type in percentage (e.g. create vs destroy operations) and their density through time. Up to now, in fact, we only supposed that operation density is not homogeneous introducing \textit{NOP} operations and we introduced arbitrary operations weights.

During all simulations instances where supposed to run at a maximum workload. Their resource usage, in fact, is calculated directly from their flavor. It could be interesting to simulate different workloads on instances basing on the type of application they are running. We could extend Nova's \code{fake} module to comprise a \code{DynamicWLInstance} which simulates different workloads given an application type. Workload simulation should be developed starting from data taken from the state of the art.

Up to now, our metrics doesn't involve number of migrations performed. Every live migration performed, in fact, has a cost in terms of energy and time \todo{ref to paper! download paper please}. In the future we will track the number of live migrations performed. It could be that different algorithms bring to different numbers of live migration and, thus, are preferable to others.

During the whole development we clashed with the current development of \code{nova.virt.fake.FakeDriver}. It seems that \code{fake} module of Nova wasn't and isn't in strong evolution. It seems that the community, in this moment, has to deal with greater problems, \emph{Kilo} version of OpenStack, in fact, is to be released. However we understood and we used the power of \code{fake} module and we want to fix its bugs and enhance it. During \texttt{nova-consolidator} service development, we had to fix (locally, up to now) a bug in live migration feature\footnote{\url{https://bugs.launchpad.net/nova/+bug/1426433}} and to accept the resize operation problem (see section \ref{sec:eval_cons}). We also had to implement \code{nova.virt.fake.MStandardFakeDriver} which allows the developer to start a compute node with multiples and sub-multiples of a \code{nova.virt.fake.StandardFakeDriver} (12 vCPUs, 16384 MB of RAM and 2048 GB of disk) by means of configurations files (\texttt{fake\_driver\_multiplier} option). This was necessary for us to simulate different architectures and limit in spawning instances. By default, in fact, \code{FakeDriver} offers a standard implementation with $1000$ vCPUs, which is too big to reach system saturation (or reasonable usage percentages) in short simulations and a \code{SmallFakeDriver} ($1$ vCPU), which is too small to host more than one instance\footnote{https://github.com/affear/nova/blob/n-cons/nova/virt/fake.py}. We think in the next future to open blueprints\footnote{\url{https://wiki.openstack.org/wiki/Blueprints}} both for \code{MStandardFakeDriver} and for \code{DynamicWLInstance} and to improve Nova's ``fake'' implementation\footnote{A blueprint for service \texttt{nova-consolidator} is currently available at \url{https://blueprints.launchpad.net/nova/+spec/nova-consolidator}.}. It could be interesting to somehow simulate nodes' energy consumption in \code{FakeDriver} or directly out of OpenStack. Up to now, it would be a nonsense to extract nodes' energy consumption, given that our nodes are virtualized into Docker containers.

In section \ref{sec:eval_adock}, we have already said that OpenStack, by default, prefers virtual machine spreading over stacking. It would be interesting to set \texttt{ram\_weight\_multiplier} to a negative value in the way to make weighers prefer stacking over spreading\footnote{http://docs.openstack.org/developer/nova/devref/filter\_scheduler.html\#weights}. In this way OpenStack would be much better at virtual machine placement in a perspective of consolidation. We could start simulations with a fixed value of instances, e.g. $30$, and perform only destroy and \textit{NOP} operations and compare different consolidators in this perspective. It is useful to see consolidators in action starting from an empty system and experimenting their effect on create operations. However, if placement is performed with a consolidation perspective, it is when instances are deleted that consolidation makes the difference filling the ``holes'' left by them.