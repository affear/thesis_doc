%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% CONCLUSIONS
%%--------------------------------------------------------------------------

Due to the two-topic nature of this thesis we split this chapter into two sections. Section \ref{sec:conc_adock} discusses about conclusions on results obtained from tests on aDock system (see section \ref{sec:eval_adock}) and possible future work on the system itself. Section \ref{sec:conc_cons} discusses conclusions on results obtained from consolidation algorithms testing (see section \ref{sec:eval_cons}) and future work in the field of virtual machine consolidation in OpenStack.

\section{aDock}
\label{sec:conc_adock}
Tests conducted on aDock system confirmed our suppositions on aDock. It is possible for a simple developer or researcher to develop on its laptop and run simulations against an OpenStack system using aDock. Results obtained give reasonable starting times for the entire architecture. We can say that aDock is ``lightweight''. However we think that a comparison between aDock and one of the other options available at the state of the art (e.g. \textit{Chef}. See subsection \ref{sub:sota_chef}) would be a must. Direct comparison is necessary to understand if aDock is really better then its ``competitors''. To our detriment there is to say that, by construction, containers make aDock more lightweight than an architecture with hypervisor and virtual machines (see paragraph \ref{p:nfr2}) \todo{``figura dei cioccolatai''??}.

Future work on aDock is vast. For what concerns FakeStack, it could become a \emph{modular} system. Docker developers strongly advocate small, lightweight containers where each container has a single responsibility. This is not the case in FakeStack, which gives a lot of responsibilities to a single container. FakeStack nodes are ``fat containers'' that run a lot of different processes. The controller node, for example, in its minimal configuration, runs \texttt{rabbitmq-server}; \texttt{mysql}; \texttt{keystone}; \texttt{glance-api}; \texttt{glance-registry}; \texttt{nova-api}; \texttt{nova-cert}; \texttt{nova-conductor} and \texttt{nova-scheduler}. This is in total contrast with Docker philosophy and makes impossible for FakeStack to be ``flexible''. The solution to this problem would be to make each OpenStack service run in a separate container\footnote{There is already an attempt to this, \url{https://hub.docker.com/u/cosmicq/}.}. This change would make FakeStack much more flexible and configurable by the user. In this case we should provide a templating language to make FakeStack automatically deploy an OpenStack architecture as given by the user, as \textit{Chef} and \textit{Puppet} already do. According to us and with the adequate support by the OpenStack community, FakeStack could become the equivalent, but Docker-powered, of \textit{Chef-OpenStack} and \textit{Puppet-OpenStack} in the OpenStack world.

For what concerns the templating language and automate deploying of an OpenStack system, Docker recently released tools for container orchestration\footnote{\url{http://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/}} among which we can find \textit{Compose}\footnote{\url{http://docs.docker.com/compose/}} which is ``a way of defining and running multi-container distributed applications with Docker''. We think that this functionality fits perfectly with what we need into FakeStack. Compose allows the user to create a \texttt{docker-compose.yml} file and start its newly defined system running \code{docker-compose up} and Compose will start and run the entire system, determining the right order to start containers.

If we want to start a controller node, we could start it using Compose. Listing \ref{lst:compose_ctrl} shows a possible \texttt{docker-compose.yml} file for a controller node as a proof of concept\footnote{Not all necessary services are listed. Ports are avoided. Images are supposed to be available.}. Keystone (\texttt{key}) container depends on MySQL (\texttt{db}) and RabbitMQ (\texttt{rabbit}) containers, as well as Glance API (\texttt{g-api}) and Nova API (\texttt{n-api}) containers depend on \texttt{key}. All configuration files are specified as volumes to make it unnecessary to rebuild images if a modification into configuration happens.

\begin{lstlisting}[
	float,
	caption={Sample controller's \texttt{docker-compose.yml}},
	label={lst:compose_ctrl},
	tabsize=2,
	numbers=none
]
key:
  image: fs-key
  links:
   - db
   - rabbit
  ports:
   - ...
  volumes:
   - ./keystone.conf

g-api:
	image: fs-g-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./glance.conf

n-api:
	image: fs-n-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./nova.conf

db:
  image: mysql

rabbit:
  image: rabbit
\end{lstlisting}

We could also provide the user with built-in \emph{composed} system, such as ``all-in-one'' and ``1 + N'' architectures. We could also provide systems for single OpenStack modules. ``Nova'' composition, for example, could include containers for all Nova services, such as the scheduler, the API and so on.

In modular case, every image maps to a single OpenStack service. Images provided should fit users needs to choose OpenStack's running code (as already said in non-functional requirement $1$. See \ref{p:nfr1}). For this reason we think that we could use DevStack again to install each single service. This choice would allow the user to choose GitHub repository URL and branch and to configure the service itself (see subsection \ref{sub:fakestack_conf}). The user should create a different configuration file for each service (e.g. \texttt{keystone.conf} and \texttt{nova.conf}) containing the repository URL and branch used and the different installation phases of the service with their specific configuration. At this point, we could merge each different configuration file to a main \texttt{local.conf} file which specifies the different \texttt{ENABLED\_SERVICE} for each of the images.

OpenStack configuration is not ``hot-reloaded'' at every modification, thus, implies container reboot. We could avoid rebooting (rebooting is heavier then service restarting, which would imply a new DevStack installation) providing scripts to restart services inside containers. This fact is not trivial, because services could have dependencies among them and restarting could break service startup and other related services. 

For what concerns Oscard, we could give the possibility to user to run simulations with a composition of \emph{all} possible operations that OpenStack allows to perform. Only create, resize and destroy operations are supported up to now. Another point is to support aggregates of simulations. It happens that a user wants to run a group of simulations and extract averages of the aggregates already stored in Bifrost by Oscard. It was our case when we came to groups of $50$ simulations, each block with a different consolidator. To calculate the numbers in table \ref{tab:cons_vs}, we wrote a python script which extracted averages of aggregates from each group of simulations, knowing starting and ending simulation ID of each group. We suppose that a situation like this one could happen very often to users. Oscard should allow the user to give an unique label to a group of simulations and automatically extract the averages (also standard deviations would be good) of the aggregates which Oscard already calculates. Polyphemus should display those new data and represent someway the concept of \emph{group} of simulations, of course.

\section{Consolidators}
\label{sec:conc_cons}