%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% CONCLUSIONS
%%--------------------------------------------------------------------------

Due to the two-topic nature of this thesis we split this chapter into two sections. Section \ref{sec:conc_adock} is about conclusions on results obtained from tests on aDock system (see section \ref{sec:eval_adock}) and possible future work. Section \ref{sec:conc_cons} discusses conclusions on results obtained from testing on consolidation algorithms (see section \ref{sec:eval_cons}) and future work in the field of virtual machine consolidation in OpenStack.

\section{aDock}
\label{sec:conc_adock}
Tests conducted on our system confirmed our suppositions: it is possible for a simple developer or researcher to develop OpenStack code on his/her laptop and use aDock to run simulations against an OpenStack system. Results obtained give reasonable starting times for the entire architecture. We can say that aDock is ``lightweight''. However we think that a comparison between aDock and one of the other options available at the state of the art (e.g. \textit{Chef}. See subsection \ref{sub:sota_chef}) would be a must. Direct comparison is necessary to understand if aDock is really better then its ``competitors'' at startup timings. Keep in mind that, by construction, Docker containers make aDock more lightweight than an architecture with hypervisor and virtual machines (see paragraph \ref{p:nfr2}) \todo{``figura dei cioccolatai''??}.

Future work on aDock is vast. For what concerns FakeStack, it could become a \emph{modular} system. Docker developers strongly advocate small, lightweight containers where each container has a single responsibility. This is not the case in FakeStack, which gives a lot of responsibilities to a single container. FakeStack nodes are ``fat containers'' that run a lot of different processes. The controller node, for example, in its minimal configuration, runs \texttt{rabbitmq-server}; \texttt{mysql}; \texttt{keystone}; \texttt{glance-api}; \texttt{glance-registry}; \texttt{nova-api}; \texttt{nova-cert}; \texttt{nova-conductor} and \texttt{nova-scheduler}. This is in total contrast with Docker's philosophy and makes it difficult for FakeStack to be ``flexible''. The solution to this problem would be to make each OpenStack service run in a separate container\footnote{There is already an attempt to this, \url{https://hub.docker.com/u/cosmicq/}.}. This change would make FakeStack much more flexible and configurable by the user. In this case we should provide a templating language to make FakeStack automatically deploy an OpenStack architecture as given by the user, as \textit{Chef} and \textit{Puppet} already do (see subsections \ref{sub:sota_chef} and \ref{sub:sota_puppet}). According to us, and with the adequate support by the OpenStack community, FakeStack could become the equivalent, but Docker-powered, of \textit{Chef-OpenStack} and \textit{Puppet-OpenStack} in the OpenStack world.

For what concerns the templating language and automate deploying of an OpenStack system, Docker recently released tools for container orchestration\footnote{\url{http://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/}} among which we can find \textit{Compose}\footnote{\url{http://docs.docker.com/compose/}} which is ``a way of defining and running multi-container distributed applications with Docker''. We think that this functionality fits perfectly with what we need into FakeStack. Compose allows the user to create a \texttt{docker-compose.yml} file and start its newly defined system running \code{docker-compose up} and Compose will start and run the entire system, determining the right order to start containers.

If we want to start a controller node, we could start it using Compose. Listing \ref{lst:compose_ctrl} shows a possible \texttt{docker-compose.yml} file for a controller node as a proof of concept\footnote{Not all necessary services are listed. Ports are avoided. Images are supposed to be available.}. Keystone (\texttt{key}) container depends on MySQL (\texttt{db}) and RabbitMQ (\texttt{rabbit}) containers, as well as Glance API (\texttt{g-api}) and Nova API (\texttt{n-api}) containers depend on \texttt{key}. All configuration files are specified as volumes to make it unnecessary to rebuild images if a modification into configuration happens.

\begin{lstlisting}[
	float,
	caption={Sample controller's \texttt{docker-compose.yml}},
	label={lst:compose_ctrl},
	tabsize=2,
	numbers=none
]
key:
  image: fs-key
  links:
   - db
   - rabbit
  ports:
   - ...
  volumes:
   - ./keystone.conf

g-api:
	image: fs-g-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./glance.conf

n-api:
	image: fs-n-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./nova.conf

db:
  image: mysql

rabbit:
  image: rabbit
\end{lstlisting}

We could also provide the user with built-in \emph{composed} system, such as ``all-in-one'' and ``1 + N'' architectures. We could also provide systems for single OpenStack modules. ``Nova'' composition, for example, could include containers for all of the Nova's services, such as the scheduler, the API and so on.

In modular case, every image maps to a single OpenStack service. Images provided should fit users needs to choose OpenStack's running code (as already said in non-functional requirement $1$. See \ref{p:nfr1}). For this reason we think that we could still use DevStack to install each single service. This choice would allow the user to choose GitHub repository URL and branch, and to configure the service itself (see subsection \ref{sub:fakestack_conf}). For what concerns configuration, the user should still provide a main \texttt{local.conf} file as in DevStack; but we could provide a script which parses this file and generates a different configuration file for each of the services configured. The output files (e.g. \texttt{keystone.conf} and \texttt{nova.conf}) would contain global configuration for DevStack itself; the specific service's configuration (including its repository URL and branch) and the \texttt{ENABLED\_SERVICES}. This option would be set by the script to the particular service which will be run in the specific container. The script considered should be run before container starting to obtain the different configuration files.

OpenStack configuration is not ``hot-reloaded'' at every modification, thus, implies container reboot. We could avoid rebooting (rebooting is heavier then service restarting, which would imply a new DevStack installation) providing scripts to restart services inside containers. This fact is not trivial, because services could have dependencies among them and restarting could break service startup and other related services. 

For what concerns Oscard, we could give the possibility to user to run simulations with more and more operations that OpenStack allows to perform. Only create, resize and destroy operations are supported up to now.

Another point is to support aggregates of simulations. It happens that a user wants to run a group of simulations and extract averages of the aggregates already stored in Bifrost by Oscard. It was our case when we came to groups of $50$ simulations, each block with a different consolidator. To calculate the numbers in table \ref{tab:cons_vs} (see section \ref{sec:eval_cons}), we wrote a python script which extracted averages of aggregates from each group of simulations, knowing starting and ending simulation ID of each group. We suppose that a situation like this one could happen very often to users. Oscard should allow the user to give an unique label to a group of simulations and automatically extract the averages (also standard deviations would be good) of the aggregates which Oscard already calculates. Polyphemus should display those new data and represent someway the concept of \emph{group} of simulations, of course.

\section{Virtual Machine Consolidation in OpenStack}
\label{sec:conc_cons}
Tests run on different consolidators confirmed our thoughts regarding OpenStack's Virtual Machine consolidation. Consolidation brings high levels of improvement for resource usage, with respect to ``vanilla'' OpenStack. The best algorithm we found, in fact, brought a $14\%$ increase in vCPUs usage, a $20\%$ increase in RAM usage, a $1.5\%$ increase in disk usage, and a $30\%$ decrease of active nodes (on $10$ total nodes), with a maximum downscale time of about $10\%$.

In the future, we will extract standard power on and power off timings of different servers from the state of the art and compare them with maximum downscale times obtained. Maximum downscale time, in fact, is intended to be compared with those timings, given that a node, when inactive, can be powered off. If the time in which the node is inactive is too short, it could be a useless turning it off, because the system could need it while this is happening. It could be that some algorithms do not allow one to power off servers, and so, they give a ``fake'' improvement. Resource usage increases, but the system cannot exploit this efficiency trait.

We also believe that more efficient consolidation algorithms could be implemented in the future. The state of the art gives a lot of hints about this (see section \ref{sec:sota_game_theroy} and section \ref{sec:sota_mutli_agent}). 

As previously stated Oscard could run more realistic simulations. We could get data from different real cloud systems to understand how many operations are performed per second, their proportions (e.g. create vs destroy operations), and their density through time. Up until now, in fact, we only supposed that the operation density is not homogeneous (introducing \textit{NOP} operations) and we applied arbitrary proportions between operations.

During simulations instances where supposed to run at a maximum workload. It could be interesting to simulate different workloads on instances based on the type of application they are supposed to run. We could extend Nova's \code{fake} module to comprise a \code{DynamicWLInstance} which simulates different workloads given an application type. Workload simulation should be developed starting from data taken from the state of the art.

Up until now our metrics have not involved the number of migrations performed. Every live migration performed, in fact, has a cost in terms of energy and time \todo{ref to paper! download paper please}. In the future we will track the number of live migrations performed. It could be that different algorithms bring to different numbers of live migration and, thus, are preferable to others.

During the whole development we clashed with the current development of \code{nova.virt.fake.FakeDriver}. It seems that \code{fake} module of Nova isn't evolving, as are other parts of OpenStack project. The \emph{Kilo} version of OpenStack, in fact, is to be released soon and the community is highly focused on it. However, we understood and used the power of the \code{fake} module, and we want to fix its bugs and enhance it. During \texttt{nova-consolidator} service development we also had to fix a bug in the live migration feature\footnote{\url{https://bugs.launchpad.net/nova/+bug/1426433}}, as well as to extend it to accept resize operations (see section \ref{sec:eval_cons}). We also had to implement \code{nova.virt.fake.MStandardFakeDriver} which allows the developer to start a compute node with multiples and sub-multiples of a \code{nova.virt.fake.StandardFakeDriver} (12 vCPUs, 16384 MB of RAM and 2048 GB of disk) by means of configurations files (\texttt{fake\_driver\_multiplier} option)\footnote{https://github.com/affear/nova/blob/n-cons/nova/virt/fake.py}. This was necessary for us to simulate different architectures and limit spawning instances. By default, in fact, \code{FakeDriver} offers a standard implementation with $1000$ vCPUs, which is too big to reach system saturation (or reasonable usage percentages) in short simulations and a \code{SmallFakeDriver} ($1$ vCPU), which is too small to host more than one instance\footnote{https://github.com/openstack/nova/blob/master/nova/virt/fake.py}. In the near future we will open blueprints\footnote{\url{https://wiki.openstack.org/wiki/Blueprints}} for both \code{MStandardFakeDriver} and for \code{DynamicWLInstance}, to improve Nova's ``fake'' implementation\footnote{A blueprint for service \texttt{nova-consolidator} is currently available at \url{https://blueprints.launchpad.net/nova/+spec/nova-consolidator}.}. It could be interesting to somehow simulate the nodes' energy consumption in \code{FakeDriver}. For now, it would be a nonsense to extract the nodes' energy consumption, given that our nodes are virtualized Docker containers.

In section \ref{sec:eval_adock} we already said that OpenStack, by default, prefers virtual machine spreading over stacking. It would be interesting to set \texttt{ram\_weight\_multiplier} to a negative value, to make weighers prefer stacking over spreading\footnote{http://docs.openstack.org/developer/nova/devref/filter\_scheduler.html\#weights}. In this way OpenStack would be much better at virtual machine placement in a perspective of consolidation. We could start simulations with a fixed value of instances, e.g. $30$, and only perform destroy and \textit{NOP} operations, and compare different consolidators in this perspective. It would be useful to see consolidators in action starting from an empty system, and experiment their effect on create operations; however, if placement is performed with a consolidation perspective, it is when instances are deleted that consolidation makes the difference filling the ``holes'' that are left behind when instances are deleted.