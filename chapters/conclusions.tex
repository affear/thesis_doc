%!TEX root = ../thesis.tex
%%--------------------------------------------------------------------------
%% CONCLUSIONS
%%--------------------------------------------------------------------------

Due to the two-topic nature of this thesis we split this chapter into two sections. Section \ref{sec:conc_adock} contains conclusions regarding the results obtained while testing the aDock system (see section \ref{sec:eval_adock}), and possible future work. Section \ref{sec:conc_cons} discusses conclusions regarding the results obtained from testing the consolidation algorithms (see section \ref{sec:eval_cons}), as well as future work in the field of Virtual Machine consolidation in OpenStack.

\section{aDock}
\label{sec:conc_adock}
Tests conducted on our system confirmed our suppositions: it is possible for a developer or researcher to develop OpenStack code on his/her laptop and use aDock to run simulations against a fully-functional OpenStack system. The results obtained show reasonable starting times for the entire architecture. We can conclude that aDock is ``lightweight''. However we think that a comparison between aDock and one of the other options available in the state of the art (e.g. \textit{Chef}. See subsection \ref{sub:sota_chef}) should be done. Direct comparison is necessary to understand if aDock is really better then its ``competitors'' in terms of startup times. Keep in mind that, by construction, Docker containers make aDock more lightweight than an architecture that uses a hypervisor and virtual machines (see paragraph \ref{p:nfr2}).

Future work on aDock will be vast. First of all FakeStack should become a completely \emph{modular} system. Docker developers strongly advocate small, lightweight containers where each container has a single responsibility. This is not the case in FakeStack, which gives a lot of responsibilities to a single container. FakeStack nodes are ``fat containers'' that run a lot of different processes. The controller node, for example, in its minimal configuration, runs \texttt{rabbitmq-server}; \texttt{mysql}; \texttt{keystone}; \texttt{glance-api}; \texttt{glance-registry}; \texttt{nova-api}; \texttt{nova-cert}; \texttt{nova-conductor} and \texttt{nova-scheduler}. This is in contrast with Docker's philosophy and makes FakeStack less ``flexible'' than it could be. The solution to this problem would be to make each OpenStack service run in a separate container\footnote{There is already an attempt to this, \url{https://hub.docker.com/u/cosmicq/}.}. This change would make FakeStack much more flexible and configurable by the user. To do this we envision providing a templating language, one that would allow FakeStack to automatically deploy an OpenStack architecture as described by a user. This is something that \textit{Chef} and \textit{Puppet} already do (see subsections \ref{sub:sota_chef} and \ref{sub:sota_puppet}). With the adequate support from the OpenStack community, FakeStack could become the equivalent, but Docker-powered, of \textit{Chef-OpenStack} and \textit{Puppet-OpenStack} in the OpenStack world.

Docker recently released tools for container orchestration\footnote{\url{http://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/}}. Among these we have \textit{Compose}\footnote{\url{http://docs.docker.com/compose/}}, ``a way of defining and running multi-container distributed applications with Docker''. We think that this functionality fits perfectly with what we envision for a more modular FakeStack. Compose allows the user to create a \texttt{docker-compose.yml} file and start its newly defined system running \code{docker-compose up}; Compose will start and run the entire system determining the right order to start containers.

If we want to start a controller node, we could start it using Compose. Listing \ref{lst:compose_ctrl} shows a possible \texttt{docker-compose.yml} file for a controller node as a proof of concept\footnote{Not all necessary services are listed. Ports are avoided. Images are supposed to be available.}. The Keystone (\texttt{key}) container depends on the MySQL (\texttt{db}) and the RabbitMQ (\texttt{rabbit}) containers; while Glance API (\texttt{g-api}) and Nova API (\texttt{n-api}) containers depend on \texttt{key}. All configuration files are specified as volumes, to make it unnecessary to rebuild images if a modification in the configuration happens.

\begin{lstlisting}[
	float,
	caption={Sample controller's \texttt{docker-compose.yml}},
	label={lst:compose_ctrl},
	tabsize=2,
	numbers=none
]
key:
  image: fs-key
  links:
   - db
   - rabbit
  ports:
   - ...
  volumes:
   - ./keystone.conf

g-api:
	image: fs-g-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./glance.conf

n-api:
	image: fs-n-api
	links:
	 - key
	ports:
	 - ...
	volumes:
   - ./nova.conf

db:
  image: mysql

rabbit:
  image: rabbit
\end{lstlisting}

We could also provide the user with built-in \emph{composed} system, such as ``all-in-one'' and ``1 + N'' architectures. We could also provide systems for single OpenStack modules. ``Nova'' composition, for example, could include containers for all of the Nova's services, such as the scheduler, the API and so on.

In this modular case, every image will map to a single OpenStack service. The images we provide should fit the users' needs, and allow one to specify what OpenStack code to run (as already said in non-functional requirement $1$. See \ref{p:nfr1}). This is why we think that we could still use DevStack to install the single services. This choice would allow the user to choose the GitHub repository URL and branch, and to configure the service itself (see subsection \ref{sub:fakestack_conf}). Regarding configuration, the user should still provide a main \texttt{local.conf} as for DevStack; but we could provide a script which parses this file and generates a different configuration file for each of the services configured. The output files (e.g. \texttt{keystone.conf} and \texttt{nova.conf}) would contain global configurations for DevStack itself, the specific service's configuration (including its repository URL and branch), and the \texttt{ENABLED\_SERVICES}. This option would be set by the script to the particular service which will be run in the specific container. The script considered should be run before container starting to obtain the different configuration files.

OpenStack configuration is not ``hot-reloaded'' at every modification, but requires container reboot. We could avoid rebooting (rebooting is heavier then service restarting, which would imply a new DevStack installation) providing scripts to restart services inside containers. This fact is not trivial, because services could have dependencies among them and restarting could break service startup and other related services. 

Regarding Oscard, we would like to allow the user to run simulations with more operations. Currently we only support create, resize, destroy and \textit{NOP} operations.

We would also like to support aggregates of simulations. We would like to be able to run a group of simulations and extract averages of the aggregates that are stored in Bifrost. This was what we needed when we wanted to run groups of $50$ simulations, each block with a different consolidation algorithm. To calculate the numbers in table \ref{tab:cons_vs} (see section \ref{sec:eval_cons}), we wrote a python script that extracted averages of aggregates from each group of simulations, knowing the starting and ending simulation ID of each group. We suppose that such a situation could happen often to users. Oscard should allow the user to give an unique label to a group of simulations and automatically extract the averages and standard deviations of the aggregates that Oscard already calculates. Polyphemus would also need to be updated to display the new data and somehow represent the concept of \emph{groups} of simulations.

\section{Virtual Machine Consolidation in OpenStack}
\label{sec:conc_cons}
Tests run on different consolidators confirmed what we thought about OpenStack's virtual machine consolidation. This practice introduces high resource usage improvements with respect to ``vanilla'' OpenStack. The best algorithm found, in fact, brought to about $14\%$ increase in vCPUs usage; about $20\%$ increase in RAM usage; about $1.5\%$ increase in disk usage and a decrease of about $3$ active nodes and so, of a $30\%$ (on $10$ total nodes) with a maximum downscale time of about $10\%$.

In the future, we will extract standard power on and power off timings of different servers from the state of the art and compare them with maximum downscale times obtained. Maximum downscale time, in fact, is intended to be compared with those timings, given that a node, when inactive, can be powered off. If the time in which the node is inactive is too short, it could be a nonsense to turn it off, because the system could need it while this is happening. It could be that some algorithms doesn't allow power off and so, they give a ``fake'' improvement. Resource usage increases, but the system cannot exploit this efficiency trait being impossible for it to turn a server off.

More efficient consolidation algorithms could be implemented in the future. The state of the art gives a lot of hints about this (see section \ref{sec:sota_game_theroy} and section \ref{sec:sota_mutli_agent}). 

Oscard could run more realistic simulations. We could get data from different real cloud systems to understand how many operations are performed per second; their proportions (e.g. create vs destroy operations) and their density through time. Up to now, in fact, we only supposed that the operation density is not homogeneous (introducing \textit{NOP} operations) and we applied arbitrary proportions between operations.

During all simulations, instances where supposed to run at a maximum workload. It could be interesting to simulate different workloads on instances basing on the type of application they are supposed to run. We could extend Nova's \code{fake} module to comprise a \code{DynamicWLInstance} which simulates different workloads given an application type. Workload simulation should be developed starting from data taken from the state of the art.

Up to now, our metrics doesn't involve number of migrations performed. Every live migration performed, in fact, has a cost in terms of energy and time \todo{ref to paper! download paper please}. In the future we will track the number of live migrations performed. It could be that different algorithms bring to different numbers of live migration and, thus, are preferable to others.

During the whole development we clashed with the current development of \code{nova.virt.fake.FakeDriver}. It seems that \code{fake} module of Nova isn't evolving; \emph{Kilo} version of OpenStack, in fact, is to be released and the community is highly focused on it. However we understood and we used the power of \code{fake} module and we want to fix its bugs and enhance it. During \texttt{nova-consolidator} service development, we had to fix (locally, up to now) a bug in live migration feature\footnote{\url{https://bugs.launchpad.net/nova/+bug/1426433}} and to accept the resize operation problem (see section \ref{sec:eval_cons}). We also had to implement \code{nova.virt.fake.MStandardFakeDriver} which allows the developer to start a compute node with multiples and sub-multiples of a \code{nova.virt.fake.StandardFakeDriver} (12 vCPUs, 16384 MB of RAM and 2048 GB of disk) by means of configurations files (\texttt{fake\_driver\_multiplier} option)\footnote{https://github.com/affear/nova/blob/n-cons/nova/virt/fake.py}. This was necessary for us to simulate different architectures and limit in spawning instances. By default, in fact, \code{FakeDriver} offers a standard implementation with $1000$ vCPUs, which is too big to reach system saturation (or reasonable usage percentages) in short simulations and a \code{SmallFakeDriver} ($1$ vCPU), which is too small to host more than one instance\footnote{https://github.com/openstack/nova/blob/master/nova/virt/fake.py}. We think in the next future to open blueprints\footnote{\url{https://wiki.openstack.org/wiki/Blueprints}} both for \code{MStandardFakeDriver} and for \code{DynamicWLInstance} and to improve Nova's ``fake'' implementation\footnote{A blueprint for service \texttt{nova-consolidator} is currently available at \url{https://blueprints.launchpad.net/nova/+spec/nova-consolidator}.}. It could be interesting to somehow simulate nodes' energy consumption in \code{FakeDriver} or directly out of OpenStack. Up to now, it would be a nonsense to extract nodes' energy consumption, given that our nodes are virtualized into Docker containers.

In section \ref{sec:eval_adock}, we have already said that OpenStack, by default, prefers virtual machine spreading over stacking. It would be interesting to set \texttt{ram\_weight\_multiplier} to a negative value in the way to make weighers prefer stacking over spreading\footnote{http://docs.openstack.org/developer/nova/devref/filter\_scheduler.html\#weights}. In this way OpenStack would be much better at virtual machine placement in a perspective of consolidation. We could start simulations with a fixed value of instances, e.g. $30$, and perform only destroy and \textit{NOP} operations and compare different consolidators in this perspective. In fact, it is useful to see consolidators in action starting from an empty system and experimenting their effect on create operations; however, if placement is performed with a consolidation perspective, it is when instances are deleted that consolidation makes the difference filling the ``holes'' left by them.